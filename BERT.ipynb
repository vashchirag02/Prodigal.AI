{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author Name: Yash Kashyap\n",
    "\n",
    "Date 6th March 2025\n",
    "\n",
    "Topic: Explaining BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT: Bidirectional Encoder Representations from Transformers Analysis and Implementation\n",
    "BERT represents a revolutionary approach in natural language processing that has transformed how machines understand human language. This report provides a comprehensive analysis of BERT and includes implementation code that can be executed in a Jupyter notebook environment.\n",
    "\n",
    "Understanding BERT: Core Concepts and Architecture\n",
    "What is BERT?\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a language model introduced by Google in 2018 through their paper \"Pre-training of deep bidirectional transformers for language understanding\". The model achieved state-of-the-art performance in various NLP tasks including question-answering, natural language inference, classification, and general language understanding evaluation (GLUE).\n",
    "\n",
    "BERT's release followed other significant NLP models of 2018, including:\n",
    "\n",
    "ULM-Fit (January)\n",
    "\n",
    "ELMo (February)\n",
    "\n",
    "OpenAI GPT (June)\n",
    "\n",
    "BERT (October)\n",
    "\n",
    "Key Architectural Features\n",
    "BERT's architecture is distinguished by several innovative features:\n",
    "\n",
    "Bidirectional Context Processing\n",
    "Unlike previous models that processed text sequentially (left-to-right or right-to-left), BERT processes context from both directions simultaneously. This bidirectionality allows the model to develop a richer understanding of language by considering the entire context surrounding each word.\n",
    "\n",
    "Transformer-Based Architecture\n",
    "BERT utilizes the Transformer architecture, which employs self-attention mechanisms instead of recurrent neural networks. This approach:\n",
    "\n",
    "Enables better handling of long-term dependencies\n",
    "\n",
    "Allows parallel processing of all words in a sentence\n",
    "\n",
    "Improves computational efficiency compared to sequential models\n",
    "\n",
    "Training Paradigm\n",
    "BERT implements a two-stage approach to learning:\n",
    "\n",
    "Pre-training: Training on large unlabeled text corpora to learn general language understanding\n",
    "\n",
    "Fine-tuning: Adapting the pre-trained model to specific downstream tasks with labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: BERT for Sentiment Analysis using PyTorch and Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "%pip install transformers torch pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Installs the required libraries: Hugging Face Transformers for pre-trained models, PyTorch for deep learning, and pandas/numpy for data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Imports the necessary modules: BertTokenizer converts text to tokens that BERT can process, and BertForSequenceClassification is the BERT model adapted for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Loads a pre-trained BERT model and its tokenizer. The bert-base-uncased model has 12 transformer layers and processes text as lowercase. We specify num_labels=2 for binary classification (positive/negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset (replace with your own data)\n",
    "texts = [\"I love this product, it works great!\", \n",
    "         \"This movie was fantastic and entertaining\",\n",
    "         \"The service was terrible and disappointing\",\n",
    "         \"I would not recommend this restaurant\",\n",
    "         \"The experience exceeded my expectations\"]\n",
    "labels = [1, 1, 0, 0, 1]  # 1 for positive, 0 for negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Creates a small example dataset with five sentences and corresponding sentiment labels. In a real application, you would replace this with your actual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the text data\n",
    "encoded_data = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Converts text to BERT's input format:\n",
    "\n",
    "* padding=True ensures all sequences have equal length by adding padding tokens\n",
    "\n",
    "* truncation=True cuts sequences that exceed BERT's maximum length (typically 512 tokens)\n",
    "\n",
    "* return_tensors='pt' returns PyTorch tensors\n",
    "\n",
    "* input_ids are the numerical IDs representing tokens\n",
    "\n",
    "* attention_mask indicates which tokens are actual content (1) versus padding (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Creates a PyTorch dataset combining inputs, attention masks, and labels, then wraps it in a DataLoader that handles batching and shuffling. The batch size of 2 means the model will process 2 examples simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Initializes the AdamW optimizer (Adam with weight decay correction) with a learning rate of 5e-5, which is recommended for fine-tuning BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        batch_input_ids, batch_attention_masks, batch_labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}, Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: This is the fine-tuning loop:\n",
    "\n",
    "1. Places model on GPU if available for faster training\n",
    "\n",
    "2. Sets model to training mode to enable gradient computation\n",
    "\n",
    "3. For each epoch:\n",
    "\n",
    "Processes each batch of data\n",
    "\n",
    "Clears gradients with optimizer.zero_grad()\n",
    "\n",
    "Computes model outputs and loss\n",
    "\n",
    "Backpropagates gradients with loss.backward()\n",
    "\n",
    "Updates model parameters with optimizer.step()\n",
    "\n",
    "Tracks and reports the average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for making predictions\n",
    "def predict_sentiment(text):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
    "\n",
    "# Test the model\n",
    "test_texts = [\"I really enjoyed this experience\", \"This was a complete waste of time\"]\n",
    "for text in test_texts:\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {predict_sentiment(text)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "* predict_sentiment function handles sentiment prediction for new text:\n",
    "\n",
    "1. Sets model to evaluation mode (disables dropout, etc.)\n",
    "\n",
    "2. Tokenizes and encodes the input text\n",
    "\n",
    "3. Uses torch.no_grad() to disable gradient tracking for efficiency\n",
    "\n",
    "4. Runs the model to get logits (raw prediction scores)\n",
    "\n",
    "5. Converts logits to a class prediction using argmax\n",
    "\n",
    "6. Returns \"Positive\" or \"Negative\" based on the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: BERT with TensorFlow for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "%pip install tensorflow tensorflow-hub tensorflow-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Installs TensorFlow, TensorFlow Hub (for accessing pre-trained models), and TensorFlow Text (for text preprocessing operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install --upgrade tensorflow keras tensorflow_hub tensorflow_text --force-reinstall\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # for AdamW optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load IMDB dataset as an example\n",
    "imdb_dataset = tf.keras.utils.get_file(\n",
    "    'aclImdb_v1.tar.gz', \n",
    "    'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\n",
    "    untar=True, cache_dir='.', cache_subdir='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Downloads and extracts the IMDB movie review dataset, which contains 50,000 reviews labeled as positive or negative. This popular benchmark is used for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dataset\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test',\n",
    "    batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Creates TensorFlow datasets from the downloaded IMDB dataset:\n",
    "\n",
    "1. Sets batch size to 32 and fixes random seed for reproducibility\n",
    "\n",
    "2. Creates training dataset (80% of training data)\n",
    "\n",
    "3. Creates validation dataset (20% of training data)\n",
    "\n",
    "4. Creates test dataset from the separate test directory\n",
    "\n",
    "5. The directory structure is used to automatically assign labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading BERT from TensorFlow Hub\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-4_H-512_A-8': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-4_H-512_A-8': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Configures loading of BERT from TensorFlow Hub:\n",
    "\n",
    "1. Selects a smaller BERT variant (4 layers instead of 12) for faster training\n",
    "\n",
    "2. Maps model names to their TensorFlow Hub URLs\n",
    "\n",
    "3. Gets handles for both the BERT encoder and its preprocessing component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build preprocessing model\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "\n",
    "# Build BERT model\n",
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)\n",
    "\n",
    "classifier_model = build_classifier_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Builds a classification model using BERT:\n",
    "\n",
    "1. Creates an input layer accepting raw text strings\n",
    "\n",
    "2. Adds the BERT preprocessing layer to handle tokenization\n",
    "\n",
    "3. Connects the BERT encoder layer with trainable=True to allow fine-tuning\n",
    "\n",
    "4. Uses the pooled output from BERT (representation of the entire sequence)\n",
    "\n",
    "5. Adds dropout (0.1) to prevent overfitting\n",
    "\n",
    "6. Adds a single output neuron for binary classification\n",
    "\n",
    "7. Returns the assembled Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define loss and metrics\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "\n",
    "# Define optimizer with weight decay\n",
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(raw_train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1 * num_train_steps)\n",
    "\n",
    "optimizer = optimization.create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    optimizer_type='adamw')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Sets up the optimizer with learning rate scheduling:\n",
    "\n",
    "1. Calculates total training steps (batches per epoch × number of epochs)\n",
    "\n",
    "2. Allocates 10% of steps for learning rate warm-up\n",
    "\n",
    "3. Creates an AdamW optimizer with:\n",
    "\n",
    "- Initial learning rate of 3e-5\n",
    "\n",
    "- Learning rate warm-up phase\n",
    "\n",
    "- Learning rate decay after warm-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Train the model\n",
    "history = classifier_model.fit(\n",
    "    raw_train_ds,\n",
    "    validation_data=raw_val_ds,\n",
    "    epochs=epochs)\n",
    "\n",
    "# Plot training results\n",
    "def plot_history(history):\n",
    "    acc = history.history['binary_accuracy']\n",
    "    val_acc = history.history['val_binary_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Creates visualization function for training metrics:\n",
    "\n",
    "1. Extracts accuracy and loss values from training history\n",
    "\n",
    "2. Creates a figure with two subplots (accuracy and loss)\n",
    "\n",
    "3. Plots training metrics in blue and validation metrics in red\n",
    "\n",
    "4. Adds titles, labels, and legends for clarity\n",
    "\n",
    "5. This visualization helps monitor model performance and detect overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment of new texts\n",
    "def predict_sentiment_tf(model, texts):\n",
    "    results = model.predict(tf.constant(texts))\n",
    "    return [(text, \"Positive\" if score > 0 else \"Negative\") \n",
    "            for text, score in zip(texts, results)]\n",
    "\n",
    "# Test predictions\n",
    "example_texts = [\n",
    "    \"This movie was excellent! I loved it.\",\n",
    "    \"The acting was terrible and the plot made no sense.\"\n",
    "]\n",
    "\n",
    "predictions = predict_sentiment_tf(classifier_model, example_texts)\n",
    "for text, sentiment in predictions:\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "1. predict_sentiment_tf function:\n",
    "\n",
    "2. Takes a model and list of texts\n",
    "\n",
    "3. Converts texts to TensorFlow constants\n",
    "\n",
    "4. Gets raw prediction scores from model\n",
    "\n",
    "5. Classifies as \"Positive\" if score > 0, otherwise \"Negative\"\n",
    "\n",
    "6. Returns pairs of (text, sentiment)\n",
    "\n",
    "7. Tests the function on positive and negative example texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT Applications and Use Cases\n",
    "BERT has demonstrated impressive performance across numerous NLP tasks:\n",
    "\n",
    "Sentiment Analysis\n",
    "The model can accurately classify text sentiment, making it valuable for analyzing customer reviews, social media content, and market sentiment.\n",
    "\n",
    "Text Classification\n",
    "BERT excels at categorizing text into predefined classes, useful for content organization, topic modeling, and intent classification.\n",
    "\n",
    "Question Answering\n",
    "The model can extract answers from text passages, powering intelligent Q&A systems and information retrieval applications.\n",
    "\n",
    "Named Entity Recognition\n",
    "BERT can identify and classify named entities (people, organizations, locations) within text, supporting information extraction systems.\n",
    "\n",
    "Language Understanding\n",
    "The model's bidirectional nature enables nuanced understanding of language context, improving performance in tasks requiring semantic comprehension.\n",
    "\n",
    "Advantages of BERT\n",
    "Bidirectional Context Understanding\n",
    "BERT's ability to process text in both directions simultaneously provides a more comprehensive understanding of language than unidirectional models.\n",
    "\n",
    "Transfer Learning Efficiency\n",
    "Pre-trained on massive text corpora, BERT can be fine-tuned for specific tasks with relatively small amounts of labeled data, making it efficient for specialized applications.\n",
    "\n",
    "Parallelization\n",
    "Unlike recurrent neural networks, BERT can process all words in a sentence simultaneously, significantly improving computational efficiency.\n",
    "\n",
    "Conclusion\n",
    "BERT represents a significant advancement in natural language processing, offering powerful contextual language understanding through its innovative bidirectional transformer architecture. Its ability to be fine-tuned for specific tasks while leveraging knowledge from pre-training on massive text corpora makes it exceptionally versatile and effective across a wide range of NLP applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
